# Data_Analysis_Python
The README file contains the description of the following projects:

## 1. Black Friday Sales

  Library used: Numpy, Pandas, Seaborn, Matplotlib

 About Dataset:
 - Black Friday is a shopping holiday  that takes place on the day after Thanksgiving. It is known for its deep discounts and special deals on a wide range of products, including electronics, home goods, clothing, and more. Many retailers offer special doorbuster deals and extended hours on Black Friday, and it is traditionally one of the busiest shopping days of the year. 
- The dataset that we are having contains 537578 rows and 12 columns.
- Our main goal will be to do various kind of analysis and get inferences which will be able to help to provide valuable insights on the market and hence will be beneficial for the company.
 
 We are going to divide the project into 7 parts:
   1. Dataset Walkthrough
   2. Analyzing Columns
   3. Analyzing Gender
   4. Analysing Age & Marital Status
   5. Multi Column Analysis
   6. Occupation and Products Analysis
   7. Combining Gender & Marital Status

------------------------------------------------------------

## 2.	GDP Analysis

  Library used: Numpy, Pandas, Seaborn, Matplotlib, Plotly

 About Dataset:
 - Gross Domestic Product (GDP) is a measure of the size and health of an economy. It represents the total value of all goods and services produced within a country over a specific period of time, usually a year. GDP is considered a key indicator of a country's economic performance and is used to compare the economic output of different countries.
- There are several ways to analyze GDP data. One way is to compare GDP growth over time to get a sense of how an economy is expanding or contracting. Another way is to compare GDP per capita, which is GDP divided by the population of a country, to get a sense of the average standard of living in a country.
 - Country, regional and world GDP in current US Dollars ($). Regional means collections of countries e.g. Europe & Central Asia.
 - The data is sourced from the World Bank, which in turn lists as sources: World Bank national accounts data, and OECD National Accounts data files.

  We are going to divide the project into 7 parts:
   1. Dataset Walkthrough
   2. GDP Growth of a Country
   3. GDP Growth of whole Dataset
   4. Plotting Graph using Plotly
   5. Plotting Graph of each Country wrt to world (80T)
   6. GDP across countries Comparison
   7. Compare GDP growth for different countries

----------------------------------------------------------

## 3.	Heart Disease Visualization

 Library used: Numpy, Pandas, Seaborn, Matplotlib
 
 About Dataset:
 - This data set dates from 1988 and consists of four databases: Cleveland, Hungary, Switzerland, and Long Beach V.
 - It contains 76 attributes, including the predicted attribute, but all published experiments refer to using a subset of 14 of them.
 - The "target" field refers to the presence of heart disease in the patient. It is integer valued 0 = no disease and 1 = disease.
 - Data visualization is a powerful tool for understanding and communicating insights from a dataset. It can help you to identify patterns, trends, and relationships in your data, and can be a useful way to communicate your findings to others.
 - There are many different types of visualizations that you can use, and the best choice for a particular dataset will depend on the nature of the data and the insights you are trying to convey.
 - Some common types of visualizations for exploring a heart disease dataset include scatter plots, line plots, bar plots, and histograms.

 We are going to divide the project into 6 parts:
  1. Heart Disease EDA - Age (DistPlot)
  2. Heart Disease EDA - Categorical Columns (Pie Charts)
  3. Heart Disease EDA - ViolinPlot
  4. Heart Disease EDA - Correlation (HeatMap)
  5. Heart Disease EDA - Corrlation (PairPlot)
  6. Heart Disease EDA - Correlation - (JointPlot)
 
-----------------------------------------------------------------

## 4.	Sugarcane Production

  Library used: Numpy, Pandas, Seaborn, Matplotlib

  About Dataset:
  - Worldwide 1,889,268,880 tones of sugarcane is produced per year.
  - Brazil is the largest sugarcane producer in the world with 768,678,382 tones production volume per year.
  - India comes second with 348,448,000 tones yearly production.
  - Brazil and India produce together 59% of World's total.

 We are going to divide the project into 6 parts:
   1. Dataset Walkthrough
   2. Data Cleaning
   3. Univariate Analysis
   4. Bivariate Analysis
   5. Correlation
   6. Analysis for Continent

------------------------------------------------------------------

## 5.	Walmart Analytics

 Library used: Numpy, Pandas, Seaborn, Matplotlib
 
 About Dataset:
  - Explore the world of retail analytics with the Walmart Supermarket Sales Prediction Dataset.
  - This dataset provides historical sales data for 45 Walmart stores, each containing multiple departments.
  - Your task is to predict department-wide sales for each store, taking into account various factors such as promotional markdowns and holiday events.
  - The dataset contains the three dataset.

 We are going to divide the project into 7 parts:
   1. Dataset Walkthrough
   2. Data Cleaning
   3. Data Merging (creating a one datafram)
   4. Visualizing weekly sales
   5. Analyzing Weekly Sales per Stores
   6. Analyzing Weekly Sales per Department
   7. Correlation

------------------------------------------------------------------

## 6.	My Uber Drives

 Library used: Numpy, Pandas, Seaborn, Matplotlib

 About Dataset:
 - Here are the details of my Uber Drives of 2016. I am sharing this dataset for data science community to learn from the behavior of an ordinary Uber customer.
 - Time period: January - December 2016
 - Unit of analysis: Drives
 - Total Drives: 1,155
 - Total Miles: 12,204
 - Dataset: The dataset contains Start Date, End Date, Start Location, End Location, Miles Driven and Purpose of drive (Business, Personal, Meals, Errands, Meetings, Customer Support etc.)

 We are going to divide the project into 6 parts:
   1. Dataset Walkthrough
   2. Data Cleaning
   3. Analyzing most frequent start location, stop location and frequent miles
   4. Creating a smaller datafrmes to check for outliers and round trip
   5. Visualizing cab booking frequency over a months
   6. Analyzing majority of time cabs booked

------------------------------------------------------------------

